{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this project, I will use an Alternating Least Squares (ALS) algorithm with Spark APIs to predict the ratings for the movies in [MovieLens Datasets](https://grouplens.org/datasets/movielens/latest/)\n",
    "\n",
    "## [Recommender system](https://en.wikipedia.org/wiki/Recommender_system)\n",
    "A recommendation system is basically an information filtering system that seeks to predict the \"rating\" or \"preference\" a user would give to an item. It is widely used in different internet / online business such as Amazon, Netflix, Spotify, or social media like Facebook and Youtube. By using recommender systems, those companies are able to provide better or more suited products/services/contents that are personalized to a user based on his/her historical consumer behaviors\n",
    "\n",
    "Recommender systems typically produce a list of recommendations through collaborative filtering or through content-based filtering\n",
    "\n",
    "This project will focus on collaborative filtering and use Alternating Least Squares (ALS) algorithm to make movie predictions\n",
    "\n",
    "\n",
    "## [Alternating Least Squares](https://endymecy.gitbooks.io/spark-ml-source-analysis/content/%E6%8E%A8%E8%8D%90/papers/Large-scale%20Parallel%20Collaborative%20Filtering%20the%20Netflix%20Prize.pdf)\n",
    "ALS is one of the low rank matrix approximation algorithms for collaborative filtering. ALS decomposes user-item matrix into two low rank matrixes: user matrix and item matrix. In collaborative filtering, users and products are described by a small set of latent factors that can be used to predict missing entries. And ALS algorithm learns these latent factors by matrix factorization\n",
    "\n",
    "\n",
    "## Data Sets\n",
    "I use [MovieLens Datasets](https://grouplens.org/datasets/movielens/latest/).\n",
    "This dataset (ml-latest.zip) describes 5-star rating and free-text tagging activity from [MovieLens](http://movielens.org), a movie recommendation service. It contains 27753444 ratings and 1108997 tag applications across 58098 movies. These data were created by 283228 users between January 09, 1995 and September 26, 2018. This dataset was generated on September 26, 2018.\n",
    "\n",
    "Users were selected at random for inclusion. All selected users had rated at least 1 movies. No demographic information is included. Each user is represented by an id, and no other information is provided.\n",
    "\n",
    "The data are contained in the files `genome-scores.csv`, `genome-tags.csv`, `links.csv`, `movies.csv`, `ratings.csv` and `tags.csv`.\n",
    "\n",
    "## Project Content\n",
    "1. Load Data\n",
    "2. Spark SQL and OLAP\n",
    "3. Spark ALS based approach for training model\n",
    "4. ALS Model Selection and Evaluation\n",
    "5. Model testing\n",
    "6. Make movie recommendation to myself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "# spark imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import UserDefinedFunction, explode, desc\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "\n",
    "# data science imports\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# visualization imports\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/02 15:39:33 WARN Utils: Your hostname, harsh-raj-Mi-NoteBook-14 resolves to a loopback address: 127.0.1.1; using 10.150.43.173 instead (on interface wlp0s20f3)\n",
      "23/12/02 15:39:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/02 15:39:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# spark config\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"movie recommendation\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"96g\") \\\n",
    "    .config(\"spark.driver.memory\", \"96g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.master\", \"local[12]\") \\\n",
    "    .getOrCreate()\n",
    "# get spark context\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "movies = spark.read.load(\"data.csv\", format='csv', header=True, inferSchema=True)\n",
    "ratings = spark.read.load(\"final_ratings.csv\", format='csv', header=True, inferSchema=True)\n",
    "links = spark.read.load(\"data.csv\", format='csv', header=True, inferSchema=True)\n",
    "tags = spark.read.load(\"new_data.csv\", format='csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+--------------------+------+----------+-----+--------+--------+--------------------+------+-------------+--------------------+--------------------+\n",
      "|                name|            overview|    id|         poster_path|rating|vote_count|adult|director|producer|             creator|actors|year_released|              genres|            keywords|\n",
      "+--------------------+--------------------+------+--------------------+------+----------+-----+--------+--------+--------------------+------+-------------+--------------------+--------------------+\n",
      "|          Tagesschau|German daily news...| 94722|https://image.tmd...| 7.094|       171|False|      []|      []|                  []|    []|       1952.0|            ['News']|['German', 'daily...|\n",
      "|       Elas por Elas|Seven friends who...|219109|https://image.tmd...| 6.533|        15|False|      []|      []|['Alessandro Mars...|    []|       2023.0|['Comedy', 'Myste...|['Seven', 'friend...|\n",
      "|Temptation Island...|Relationships ka ...|238766|https://image.tmd...|   8.0|         2|False|      []|      []|    ['Jessu George']|    []|       2023.0|         ['Reality']|['Relationships',...|\n",
      "+--------------------+--------------------+------+--------------------+------+----------+-----+--------+--------+--------------------+------+-------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+------+------+\n",
      "|          name| tvId|userId|rating|\n",
      "+--------------+-----+------+------+\n",
      "|Rick and Morty|60625|   106|   9.0|\n",
      "|Rick and Morty|60625|  1576|  10.0|\n",
      "|Grey's Anatomy| 1416|     3|   1.0|\n",
      "+--------------+-----+------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+--------------------+------+----------+-----+--------+--------+--------------------+------+-------------+--------------------+--------------------+\n",
      "|                name|            overview|    id|         poster_path|rating|vote_count|adult|director|producer|             creator|actors|year_released|              genres|            keywords|\n",
      "+--------------------+--------------------+------+--------------------+------+----------+-----+--------+--------+--------------------+------+-------------+--------------------+--------------------+\n",
      "|          Tagesschau|German daily news...| 94722|https://image.tmd...| 7.094|       171|False|      []|      []|                  []|    []|       1952.0|            ['News']|['German', 'daily...|\n",
      "|       Elas por Elas|Seven friends who...|219109|https://image.tmd...| 6.533|        15|False|      []|      []|['Alessandro Mars...|    []|       2023.0|['Comedy', 'Myste...|['Seven', 'friend...|\n",
      "|Temptation Island...|Relationships ka ...|238766|https://image.tmd...|   8.0|         2|False|      []|      []|    ['Jessu George']|    []|       2023.0|         ['Reality']|['Relationships',...|\n",
      "+--------------------+--------------------+------+--------------------+------+----------+-----+--------+--------+--------------------+------+-------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "links.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+--------------------+\n",
      "|            name|    id|                tags|\n",
      "+----------------+------+--------------------+\n",
      "|      Tagesschau| 94722|News  German dail...|\n",
      "|   Elas por Elas|219109|Comedy Mystery So...|\n",
      "|Gran hermano VIP| 82250|Reality   3.818  ...|\n",
      "+----------------+------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tags.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL and OLAP\n",
    "\n",
    "Below are the questions I'd like to ask:\n",
    "1. What are the ratings?\n",
    "2. What is minimum number of ratings per user and minimum number of ratings per movie?\n",
    "3. How many movies are rated by only one user?\n",
    "4. What is the total number of users in the data sets?\n",
    "5. What is the total number of movies in the data sets?\n",
    "6. How many movies are rated by users? List movies not rated yet?\n",
    "7. List all movie genres\n",
    "8. Find out the number of movies for each category\n",
    "9. Calculate the total rating count for every movie\n",
    "10. Get a count plot for each rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the ratings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct values of ratings:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print('Distinct values of ratings:')\n",
    "print(sorted(ratings.select('rating').distinct().rdd.map(lambda r: r[0]).collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is minimum number of ratings per user and minimum number of ratings per movie?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the users that rated movies and the movies that were rated:\n",
      "Minimum number of ratings per user is 1\n",
      "Minimum number of ratings per movie is 1\n"
     ]
    }
   ],
   "source": [
    "tmp1 = ratings.groupBy(\"userID\").count().toPandas()['count'].min()\n",
    "tmp2 = ratings.groupBy(\"tvId\").count().toPandas()['count'].min()\n",
    "print('For the users that rated movies and the movies that were rated:')\n",
    "print('Minimum number of ratings per user is {}'.format(tmp1))\n",
    "print('Minimum number of ratings per movie is {}'.format(tmp2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many movies are rated by only one user?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "574 out of 865 movies are rated by only one user\n"
     ]
    }
   ],
   "source": [
    "tmp1 = sum(ratings.groupBy(\"tvId\").count().toPandas()['count'] == 1)\n",
    "tmp2 = ratings.select('tvId').distinct().count()\n",
    "print('{} out of {} movies are rated by only one user'.format(tmp1, tmp2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the total number of users in the data sets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a total of 390 distinct users in the data sets\n"
     ]
    }
   ],
   "source": [
    "tmp = ratings.select('userID').distinct().count()\n",
    "print('We have a total of {} distinct users in the data sets'.format(tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the total number of movies in the data sets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a total of 9179 distinct movies in the data sets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tmp = movies.select('id').distinct().count()\n",
    "print('We have a total of {} distinct movies in the data sets'.format(tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many movies are rated by users? List movies not rated yet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a total of 865 distinct movies that are rated by users in ratings table\n",
      "We have 8314 movies that are not rated yet\n"
     ]
    }
   ],
   "source": [
    "tmp1 = movies.select('id').distinct().count()\n",
    "tmp2 = ratings.select('tvId').distinct().count()\n",
    "print('We have a total of {} distinct movies that are rated by users in ratings table'.format(tmp2))\n",
    "print('We have {} movies that are not rated yet'.format(tmp1-tmp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List movies that are not rated yet: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|    id|                name|\n",
      "+------+--------------------+\n",
      "| 94722|          Tagesschau|\n",
      "|219109|       Elas por Elas|\n",
      "|238766|Temptation Island...|\n",
      "|209265|      Land of Desire|\n",
      "|218145|        Mom for rent|\n",
      "|213026|               Fuzue|\n",
      "|202411|Monarch: Legacy o...|\n",
      "|101463|        Al rojo vivo|\n",
      "| 91759|Come Home Love: L...|\n",
      "|226773|         Senior High|\n",
      "| 72879|    Tomorrow is Ours|\n",
      "|206559|        Binnelanders|\n",
      "|229947|  The Elegant Empire|\n",
      "|232937|     Minas de Pasión|\n",
      "| Boháč|The show describe...|\n",
      "|235484|          Suidooster|\n",
      "|   549|         Law & Order|\n",
      "|233643|Secret Mission: S...|\n",
      "|230525|Unpredictable Family|\n",
      "| 95479|      Jujutsu Kaisen|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a temp SQL table view for easier query\n",
    "movies.createOrReplaceTempView(\"movies\")\n",
    "ratings.createOrReplaceTempView(\"ratings\")\n",
    "print('List movies that are not rated yet: ')\n",
    "# SQL query (NOTE: WHERE ... NOT IN ... == ... LEFT JOIN ... WHERE ... IS NULL)\n",
    "# Approach 1\n",
    "spark.sql(\n",
    "    \"SELECT id, name \"\n",
    "    \"FROM movies \"\n",
    "    \"WHERE id NOT IN (SELECT distinct(tvId) FROM ratings)\"\n",
    ").show(20)\n",
    "# Approach 2\n",
    "# spark.sql(\n",
    "#     \"SELECT m.movieId, m.title \"\n",
    "#     \"FROM movies m LEFT JOIN ratings r ON m.movieId=r.movieId \"\n",
    "#     \"WHERE r.movieId IS NULL\"\n",
    "# ).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List all movie genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+--------------------+------+----------+--------------------+--------+--------------------+--------------------+------+-------------+--------------------+--------------------+\n",
      "|                name|            overview|    id|         poster_path|rating|vote_count|               adult|director|            producer|             creator|actors|year_released|              genres|            keywords|\n",
      "+--------------------+--------------------+------+--------------------+------+----------+--------------------+--------+--------------------+--------------------+------+-------------+--------------------+--------------------+\n",
      "|          Tagesschau|German daily news...| 94722|https://image.tmd...|     7|       171|               False|      []|                  []|                  []|    []|       1952.0|                News|['German', 'daily...|\n",
      "|       Elas por Elas|Seven friends who...|219109|https://image.tmd...|     6|        15|               False|      []|                  []|['Alessandro Mars...|    []|       2023.0|Comedy, Mystery, ...|['Seven', 'friend...|\n",
      "|Temptation Island...|Relationships ka ...|238766|https://image.tmd...|     8|         2|               False|      []|                  []|    ['Jessu George']|    []|       2023.0|             Reality|['Relationships',...|\n",
      "|      Land of Desire|When her husband ...|209265|https://image.tmd...|     6|       124|               False|      []|                  []| ['Walcyr Carrasco']|    []|       2023.0|  Drama, Crime, Soap|['husband', 'kill...|\n",
      "|        Mom for rent|Abandoned by his ...|218145|https://image.tmd...|     5|        15|               False|      []|                  []|['Matúš Libovič',...|    []|       2023.0|      Family, Comedy|['Abandoned', 'wi...|\n",
      "|               Fuzue|The department st...|213026|https://image.tmd...|     5|        21|               False|      []|                  []|    ['Gustavo Reiz']|    []|       2023.0|        Comedy, Soap|['department', 's...|\n",
      "|Monarch: Legacy o...|After surviving G...|202411|https://image.tmd...|     8|       163|               False|      []|                  []|['Matt Fraction',...|    []|       2023.0|Drama, Sci-Fi & F...|['surviving', 'Go...|\n",
      "|        Al rojo vivo|\"\"\"Al rojo vivo\"\"...|101463|https://image.tmd...|     3|        22|               False|      []|                  []|['Antonio García ...|    []|       2011.0|          News, Talk|['Al', 'rojo', 'v...|\n",
      "|Come Home Love: L...|Hung Sue Gan star...| 91759|https://image.tmd...|     5|        34|               False|      []|                  []|                  []|    []|       2017.0|Family, Comedy, D...|['Hung', 'Sue', '...|\n",
      "|         Senior High|A student’s death...|226773|https://image.tmd...|     8|         3|               False|      []|                  []|                  []|    []|       2023.0|      Drama, Mystery|['student', 'deat...|\n",
      "|    Tomorrow is Ours|The story revolve...| 72879|https://image.tmd...|     6|        45|               False|      []|                  []|['Fabienne Lesieu...|    []|       2017.0|        Crime, Drama|['story', 'revolv...|\n",
      "|      Grey's Anatomy|Follows the perso...|  1416|https://image.tmd...|     8|      9643|               False|      []|                  []|   ['Shonda Rhimes']|    []|       2005.0|               Drama|['Follows', 'pers...|\n",
      "|        Binnelanders|A South African A...|206559|https://image.tmd...|     5|        18|               False|      []|                  []|['Elsje Stark', '...|    []|       2005.0|                Soap|['South', 'Africa...|\n",
      "|  The Elegant Empire|A man and a woman...|229947|https://image.tmd...|     9|         3|               False|      []|                  []|                  []|    []|       2023.0|      Drama, Mystery|['man', 'woman', ...|\n",
      "|     Minas de Pasión|Emilia a single m...|232937|https://image.tmd...|     7|        26|               False|      []|                  []|                  []|    []|       2023.0|                    |['Emilia', 'singl...|\n",
      "|        The Simpsons|Set in Springfiel...|   456|https://image.tmd...|     7|      9259|               False|      []|                  []|   ['Matt Groening']|    []|       1989.0|Family, Animation...|['Set', 'Springfi...|\n",
      "|      Rick and Morty|Rick is a mentall...| 60625|https://image.tmd...|     8|      8820|               False|      []|                  []|['Dan Harmon', 'J...|    []|       2013.0|Animation, Comedy...|['Rick', 'mentall...|\n",
      "|The show describe...|              Jordán|  NULL|                Nikl|  NULL|      love| betrayal and ill...|   36361|https://image.tmd...|                 2.5|    16|        False|                    |                  []|\n",
      "|          Invincible|Mark Grayson is a...| 95557|https://image.tmd...|     8|      3894|               False|      []|                  []|                  []|    []|       2021.0|Animation, Sci-Fi...|['Mark', 'Grayson...|\n",
      "|          Suidooster|Suidooster is a S...|235484|https://image.tmd...|     8|         2|               False|      []|                  []|                  []|    []|       2015.0|                Soap|['Suidooster', 'S...|\n",
      "+--------------------+--------------------+------+--------------------+------+----------+--------------------+--------+--------------------+--------------------+------+-------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import round\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql.types import IntegerType\n",
    "movies = movies.dropna(subset=[\"genres\"])\n",
    "movies = movies.dropna(subset=[\"name\"])\n",
    "movies = movies.dropna(subset=[\"id\"])\n",
    "movies = movies.withColumn(\"rating\", when(col(\"rating\").isNotNull(), col(\"rating\").cast(\"int\")).otherwise(0))\n",
    "movies = movies.withColumn(\"id\", when(col(\"id\").isNotNull(), col(\"id\").cast(\"int\")).otherwise(0))\\\n",
    "# Remove square brackets and single quotes\n",
    "movies = movies.withColumn(\"genres\", regexp_replace(\"genres\", \"[\\[\\]']\", \"\"))\n",
    "movies.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All distinct genres: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct genres: 3473\n",
      "+-------------+\n",
      "|       genres|\n",
      "+-------------+\n",
      "|   unexpected|\n",
      "|          8.5|\n",
      "|      dealing|\n",
      "|      Navidad|\n",
      "|  conjunction|\n",
      "|       1974.0|\n",
      "|       washer|\n",
      "|   Jack Kenny|\n",
      "|     McCarthy|\n",
      "|          ITV|\n",
      "|       losing|\n",
      "|    sclerosus|\n",
      "|    surviving|\n",
      "|     children|\n",
      "|      cartoon|\n",
      "|     premiere|\n",
      "| Gautam Hegde|\n",
      "|       2013.0|\n",
      "|       sketch|\n",
      "| John Chernin|\n",
      "|     channels|\n",
      "|          Mae|\n",
      "|          Amy|\n",
      "|       embark|\n",
      "|          mad|\n",
      "|  Nickelodeon|\n",
      "|       Agents|\n",
      "|         STAR|\n",
      "|       Spider|\n",
      "|      Academy|\n",
      "+-------------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define a udf for splitting the genres string\n",
    "splitter = UserDefinedFunction(lambda x: x.split(','), ArrayType(StringType()))\n",
    "# query\n",
    "print('All distinct genres: ')\n",
    "num_distinct_genres = movies.select(explode(splitter(\"genres\")).alias(\"genres\")).distinct().count()\n",
    "print(f\"Number of distinct genres: {num_distinct_genres}\")\n",
    "movies.select(explode(splitter(\"genres\")).alias(\"genres\")).distinct().show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out the number of movies for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"distinct_genres.csv\"\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "movies.select(explode(splitter(\"genres\")).alias(\"genres\")).distinct().write.csv(output_path, header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of movies per genre\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|             genres|count|\n",
      "+-------------------+-----+\n",
      "|              Drama| 1935|\n",
      "|              Drama| 1922|\n",
      "|          Animation| 1587|\n",
      "|             Comedy| 1373|\n",
      "|   Sci-Fi & Fantasy| 1280|\n",
      "|             Comedy| 1230|\n",
      "| Action & Adventure| 1045|\n",
      "|            Mystery|  699|\n",
      "|                   |  638|\n",
      "| Action & Adventure|  590|\n",
      "|          Animation|  498|\n",
      "|              Crime|  479|\n",
      "|              Crime|  469|\n",
      "|             Family|  431|\n",
      "|   Sci-Fi & Fantasy|  387|\n",
      "|               Kids|  355|\n",
      "|            Reality|  350|\n",
      "|        Documentary|  269|\n",
      "|            Mystery|  178|\n",
      "|             Family|  177|\n",
      "|     War & Politics|  134|\n",
      "|             series|  129|\n",
      "|               Soap|  107|\n",
      "|               Kids|  104|\n",
      "|               Soap|  103|\n",
      "|               Talk|   93|\n",
      "|              False|   65|\n",
      "|            Reality|   61|\n",
      "|               Talk|   57|\n",
      "|        Documentary|   54|\n",
      "|            Western|   52|\n",
      "|         television|   44|\n",
      "|              aired|   39|\n",
      "|     War & Politics|   38|\n",
      "|               News|   35|\n",
      "|           produced|   31|\n",
      "|            Western|   25|\n",
      "|           episodes|   25|\n",
      "|              based|   23|\n",
      "|            Unknown|   22|\n",
      "|          broadcast|   22|\n",
      "|           American|   18|\n",
      "|                 TV|   18|\n",
      "|               News|   17|\n",
      "|               life|   17|\n",
      "|              story|   17|\n",
      "|               game|   17|\n",
      "|                sex|   17|\n",
      "|            created|   16|\n",
      "|              world|   16|\n",
      "|             school|   15|\n",
      "|           children|   14|\n",
      "|               time|   14|\n",
      "|              stars|   14|\n",
      "|            episode|   14|\n",
      "|             season|   13|\n",
      "|             2023.0|   13|\n",
      "|             called|   13|\n",
      "|                ran|   13|\n",
      "|             2021.0|   13|\n",
      "|             2011.0|   13|\n",
      "|             2018.0|   12|\n",
      "|             2001.0|   12|\n",
      "|            program|   12|\n",
      "|             2022.0|   12|\n",
      "|           animated|   12|\n",
      "|              drama|   12|\n",
      "|            January|   12|\n",
      "|             2020.0|   12|\n",
      "|         telenovela|   11|\n",
      "|               like|   11|\n",
      "|              young|   11|\n",
      "|         Television|   11|\n",
      "|             2016.0|   11|\n",
      "|             begins|   11|\n",
      "|              anime|   11|\n",
      "|                new|   11|\n",
      "|            popular|   11|\n",
      "|               best|   10|\n",
      "|           November|   10|\n",
      "|                set|   10|\n",
      "|             people|   10|\n",
      "|           featured|   10|\n",
      "|                day|   10|\n",
      "|             second|   10|\n",
      "|             United|   10|\n",
      "|             2012.0|   10|\n",
      "|              crime|   10|\n",
      "|               high|   10|\n",
      "|          premiered|   10|\n",
      "|             2013.0|    9|\n",
      "|             airing|    9|\n",
      "|               soap|    9|\n",
      "|             police|    9|\n",
      "|               body|    9|\n",
      "|              lives|    9|\n",
      "|            version|    9|\n",
      "|         production|    9|\n",
      "|            network|    9|\n",
      "|              March|    9|\n",
      "+-------------------+-----+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Counts of movies per genre')\n",
    "movies.select('id', explode(splitter(\"genres\")).alias(\"genres\")) \\\n",
    "    .groupby('genres') \\\n",
    "    .count() \\\n",
    "    .sort(desc('count')) \\\n",
    "    .show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark ALS based approach for training model\n",
    "1. Reload data\n",
    "2. Split data into train, validation, test\n",
    "3. ALS model selection and evaluation\n",
    "4. Model testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload data\n",
    "We will use an RDD-based API from pyspark.mllib to predict the ratings, so let's reload \"ratings.csv\" using sc.textFile and then convert it to the form of (user, item, rating) tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(60625, 106, 9), (60625, 1576, 10), (1416, 3, 1)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "movie_rating = sc.textFile(\"ratings_data.csv\")\n",
    "# preprocess data -- only need [\"userId\", \"movieId\", \"rating\"]\n",
    "header = movie_rating.take(1)[0]\n",
    "rating_data = movie_rating \\\n",
    "    .filter(lambda line: line!=header) \\\n",
    "    .map(lambda line: line.split(\",\")) \\\n",
    "    .map(lambda tokens: (int(tokens[0]), int(tokens[1]), int(tokens[2]))) \\\n",
    "    .cache()\n",
    "# check three rows\n",
    "rating_data.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data\n",
    "Now we split the data into training/validation/testing sets using a 6/2/2 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[203] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, validation, test = rating_data.randomSplit([6, 2, 2], seed=99)\n",
    "# cache data\n",
    "train.cache()\n",
    "validation.cache()\n",
    "test.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALS model selection and evaluation\n",
    "With the ALS model, we can use a grid search to find the optimal hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ALS(train_data, validation_data, num_iters, reg_param, ranks):\n",
    "    \"\"\"\n",
    "    Grid Search Function to select the best model based on RMSE of hold-out data\n",
    "    \"\"\"\n",
    "    # initial\n",
    "    min_error = float('inf')\n",
    "    best_rank = -1\n",
    "    best_regularization = 0\n",
    "    best_model = None\n",
    "    for rank in ranks:\n",
    "        for reg in reg_param:\n",
    "            # train ALS model\n",
    "            model = ALS.train(\n",
    "                ratings=train_data,    # (userID, productID, rating) tuple\n",
    "                iterations=num_iters,\n",
    "                rank=rank,\n",
    "                lambda_=reg,           # regularization param\n",
    "                seed=99)\n",
    "            # make prediction\n",
    "            valid_data = validation_data.map(lambda p: (p[0], p[1]))\n",
    "            predictions = model.predictAll(valid_data).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "            # get the rating result\n",
    "            ratesAndPreds = validation_data.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)\n",
    "            # get the RMSE\n",
    "            MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "            error = math.sqrt(MSE)\n",
    "            print('{} latent factors and regularization = {}: validation RMSE is {}'.format(rank, reg, error))\n",
    "            if error < min_error:\n",
    "                min_error = error\n",
    "                best_rank = rank\n",
    "                best_regularization = reg\n",
    "                best_model = model\n",
    "    print('\\nThe best model has {} latent factors and regularization = {}'.format(best_rank, best_regularization))\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/02 15:40:21 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 latent factors and regularization = 0.5: validation RMSE is 5.440518056537881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 latent factors and regularization = 0.6: validation RMSE is 5.437096470496647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 latent factors and regularization = 0.5: validation RMSE is 5.65912750855417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 latent factors and regularization = 0.6: validation RMSE is 5.661870348353117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 latent factors and regularization = 0.5: validation RMSE is 5.296479585328305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 latent factors and regularization = 0.6: validation RMSE is 5.287159275199519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 latent factors and regularization = 0.5: validation RMSE is 5.550163951479252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 latent factors and regularization = 0.6: validation RMSE is 5.536330220005044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 latent factors and regularization = 0.5: validation RMSE is 5.327023953841956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 latent factors and regularization = 0.6: validation RMSE is 5.328137927735551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 latent factors and regularization = 0.5: validation RMSE is 5.477059456249168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2222:==================================================>   (13 + 1) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 latent factors and regularization = 0.6: validation RMSE is 5.468654372619848\n",
      "\n",
      "The best model has 24 latent factors and regularization = 0.6\n",
      "Total Runtime: 96.37 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# hyper-param config\n",
    "num_iterations = 10\n",
    "ranks = [20, 22, 24, 26, 28, 30]\n",
    "reg_params = [0.5, 0.6]\n",
    "\n",
    "# grid search and select best model\n",
    "start_time = time.time()\n",
    "final_model = train_ALS(train, validation, num_iterations, reg_params, ranks)\n",
    "\n",
    "print ('Total Runtime: {:.2f} seconds'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALS model learning curve\n",
    "As we increase number of iterations in training ALS, we can see how RMSE changes and whether or not model is overfitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(arr_iters, train_data, validation_data, reg, rank):\n",
    "    \"\"\"\n",
    "    Plot function to show learning curve of ALS\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    for num_iters in arr_iters:\n",
    "        # train ALS model\n",
    "        model = ALS.train(\n",
    "            ratings=train_data,    # (userID, productID, rating) tuple\n",
    "            iterations=num_iters,\n",
    "            rank=rank,\n",
    "            lambda_=reg,           # regularization param\n",
    "            seed=99)\n",
    "        # make prediction\n",
    "        valid_data = validation_data.map(lambda p: (p[0], p[1]))\n",
    "        predictions = model.predictAll(valid_data).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "        # get the rating result\n",
    "        ratesAndPreds = validation_data.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)\n",
    "        # get the RMSE\n",
    "        MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "        error = math.sqrt(MSE)\n",
    "        # add to errors\n",
    "        errors.append(error)\n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(arr_iters, errors)\n",
    "    plt.xlabel('number of iterations')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.title('ALS Learning Curve')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create an array of num_iters\n",
    "iter_array = list(range(1,10))\n",
    "# create learning curve plot\n",
    "plot_learning_curve(iter_array, train, validation, 0.6, 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 10 iterations, alternating gradient descend starts to converge at an error around 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model testing\n",
    "And finally, make a prediction and check the testing error using out-of-sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3233:==============================================>       (12 + 2) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The out-of-sample RMSE of rating predictions is 5.496664146071237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# make prediction using test data\n",
    "test_data = test.map(lambda p: (p[0], p[1]))\n",
    "predictions = final_model.predictAll(test_data).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "# get the rating result\n",
    "ratesAndPreds = test.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)\n",
    "# get the RMSE\n",
    "MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "error = math.sqrt(MSE)\n",
    "print('The out-of-sample RMSE of rating predictions is', error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make movie recommendation to myself\n",
    "We need to define a function that takes new user's movie rating and output top 10 recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movieId(df_movies, fav_movie_list):\n",
    "    \"\"\"\n",
    "    return all movieId(s) of user's favorite movies\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_movies: spark Dataframe, movies data\n",
    "    \n",
    "    fav_movie_list: list, user's list of favorite movies\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    movieId_list: list of movieId(s)\n",
    "    \"\"\"\n",
    "    movieId_list = []\n",
    "    for movie in fav_movie_list:\n",
    "        movieIds = df_movies \\\n",
    "            .filter(movies.name.like('%{}%'.format(movie))) \\\n",
    "            .select('id') \\\n",
    "            .rdd \\\n",
    "            .map(lambda r: r[0]) \\\n",
    "            .collect()\n",
    "        movieId_list.extend(movieIds)\n",
    "    return list(set(movieId_list))\n",
    "\n",
    "\n",
    "def add_new_user_to_data(train_data, movieId_list, spark_context):\n",
    "    \"\"\"\n",
    "    add new rows with new user, user's movie and ratings to\n",
    "    existing train data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data: spark RDD, ratings data\n",
    "    \n",
    "    movieId_list: list, list of movieId(s)\n",
    "\n",
    "    spark_context: Spark Context object\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    new train data with the new user's rows\n",
    "    \"\"\"\n",
    "    # get new user id\n",
    "    new_id = train_data.map(lambda r: r[0]).max() + 1\n",
    "    # get max rating\n",
    "    max_rating = train_data.map(lambda r: r[2]).max()\n",
    "    # create new user rdd\n",
    "    user_rows = [(new_id, movieId, max_rating) for movieId in movieId_list]\n",
    "    new_rdd = spark_context.parallelize(user_rows)\n",
    "    # return new train data\n",
    "    return train_data.union(new_rdd)\n",
    "\n",
    "\n",
    "def get_inference_data(train_data, df_movies, movieId_list):\n",
    "    \"\"\"\n",
    "    return a rdd with the userid and all movies (except ones in movieId_list)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data: spark RDD, ratings data\n",
    "\n",
    "    df_movies: spark Dataframe, movies data\n",
    "    \n",
    "    movieId_list: list, list of movieId(s)\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    inference data: Spark RDD\n",
    "    \"\"\"\n",
    "    # get new user id\n",
    "    new_id = train_data.map(lambda r: r[0]).max() + 1\n",
    "    # return inference rdd\n",
    "    return df_movies.rdd \\\n",
    "        .map(lambda r: r[0]) \\\n",
    "        .distinct() \\\n",
    "        .filter(lambda x: x not in movieId_list) \\\n",
    "        .map(lambda x: (new_id, x))\n",
    "\n",
    "\n",
    "def make_recommendation(best_model_params, ratings_data, df_movies, \n",
    "                        fav_movie_list, n_recommendations, spark_context):\n",
    "    \"\"\"\n",
    "    return top n movie recommendation based on user's input list of favorite movies\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    best_model_params: dict, {'iterations': iter, 'rank': rank, 'lambda_': reg}\n",
    "\n",
    "    ratings_data: spark RDD, ratings data\n",
    "\n",
    "    df_movies: spark Dataframe, movies data\n",
    "\n",
    "    fav_movie_list: list, user's list of favorite movies\n",
    "\n",
    "    n_recommendations: int, top n recommendations\n",
    "\n",
    "    spark_context: Spark Context object\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    list of top n movie recommendations\n",
    "    \"\"\"\n",
    "    # modify train data by adding new user's rows\n",
    "    movieId_list = get_movieId(df_movies, fav_movie_list)\n",
    "    train_data = add_new_user_to_data(ratings_data, movieId_list, spark_context)\n",
    "    \n",
    "    # train best ALS\n",
    "    model = ALS.train(\n",
    "        ratings=train_data,\n",
    "        iterations=best_model_params.get('iterations', None),\n",
    "        rank=best_model_params.get('rank', None),\n",
    "        lambda_=best_model_params.get('lambda_', None),\n",
    "        seed=99)\n",
    "    \n",
    "    # get inference rdd\n",
    "    inference_rdd = get_inference_data(ratings_data, df_movies, movieId_list)\n",
    "    \n",
    "    # inference\n",
    "    predictions = model.predictAll(inference_rdd).map(lambda r: (r[1], r[2]))\n",
    "    \n",
    "    # get top n movieId\n",
    "    topn_rows = predictions.sortBy(lambda r: r[1], ascending=False).take(n_recommendations)\n",
    "    topn_ids = [r[0] for r in topn_rows]\n",
    "    \n",
    "    # return movie titles\n",
    "    return df_movies.filter(movies.movieId.isin(topn_ids)) \\\n",
    "                    .select('name') \\\n",
    "                    .rdd \\\n",
    "                    .map(lambda r: r[0]) \\\n",
    "                    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pretend I am a new user in this recommender system. I will input a handful of my all-time favorite movies into the system. And then the system should output top N movie recommendations for me to watch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):=>                               (6 + 8) / 14]\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 459, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle_fast.py\", line 632, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/mllib/recommendation.py\", line 51, in __reduce__\n",
      "    return Rating, (int(self.user), int(self.product), float(self.rating))\n",
      "                                    ^^^^^^^^^^^^^^^^^\n",
      "TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "23/12/02 15:42:57 ERROR Executor: Exception in task 5.0 in stage 3240.0 (TID 6303)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 459, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle_fast.py\", line 632, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/mllib/recommendation.py\", line 51, in __reduce__\n",
      "    return Rating, (int(self.user), int(self.product), float(self.rating))\n",
      "                                    ^^^^^^^^^^^^^^^^^\n",
      "TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 278, in dump_stream\n",
      "    bytes = self.serializer.dumps(vs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 469, in dumps\n",
      "    raise pickle.PicklingError(msg)\n",
      "_pickle.PicklingError: Could not serialize object: TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:224)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "23/12/02 15:42:57 WARN TaskSetManager: Lost task 5.0 in stage 3240.0 (TID 6303) (10.150.43.173 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 459, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle_fast.py\", line 632, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/mllib/recommendation.py\", line 51, in __reduce__\n",
      "    return Rating, (int(self.user), int(self.product), float(self.rating))\n",
      "                                    ^^^^^^^^^^^^^^^^^\n",
      "TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 278, in dump_stream\n",
      "    bytes = self.serializer.dumps(vs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 469, in dumps\n",
      "    raise pickle.PicklingError(msg)\n",
      "_pickle.PicklingError: Could not serialize object: TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:224)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n",
      "23/12/02 15:42:57 ERROR TaskSetManager: Task 5 in stage 3240.0 failed 1 times; aborting job\n",
      "23/12/02 15:42:58 WARN TaskSetManager: Lost task 9.0 in stage 3240.0 (TID 6307) (10.150.43.173 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 5 in stage 3240.0 failed 1 times, most recent failure: Lost task 5.0 in stage 3240.0 (TID 6303) (10.150.43.173 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 459, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle_fast.py\", line 632, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/mllib/recommendation.py\", line 51, in __reduce__\n",
      "    return Rating, (int(self.user), int(self.product), float(self.rating))\n",
      "                                    ^^^^^^^^^^^^^^^^^\n",
      "TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 278, in dump_stream\n",
      "    bytes = self.serializer.dumps(vs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 469, in dumps\n",
      "    raise pickle.PicklingError(msg)\n",
      "_pickle.PicklingError: Could not serialize object: TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:224)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n",
      "Driver stacktrace:)\n",
      "23/12/02 15:42:58 WARN TaskSetManager: Lost task 6.0 in stage 3240.0 (TID 6304) (10.150.43.173 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 5 in stage 3240.0 failed 1 times, most recent failure: Lost task 5.0 in stage 3240.0 (TID 6303) (10.150.43.173 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 459, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle_fast.py\", line 632, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/mllib/recommendation.py\", line 51, in __reduce__\n",
      "    return Rating, (int(self.user), int(self.product), float(self.rating))\n",
      "                                    ^^^^^^^^^^^^^^^^^\n",
      "TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 278, in dump_stream\n",
      "    bytes = self.serializer.dumps(vs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 469, in dumps\n",
      "    raise pickle.PicklingError(msg)\n",
      "_pickle.PicklingError: Could not serialize object: TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:224)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n",
      "Driver stacktrace:)\n",
      "23/12/02 15:42:58 WARN TaskSetManager: Lost task 8.0 in stage 3240.0 (TID 6306) (10.150.43.173 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 5 in stage 3240.0 failed 1 times, most recent failure: Lost task 5.0 in stage 3240.0 (TID 6303) (10.150.43.173 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 459, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle_fast.py\", line 632, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/mllib/recommendation.py\", line 51, in __reduce__\n",
      "    return Rating, (int(self.user), int(self.product), float(self.rating))\n",
      "                                    ^^^^^^^^^^^^^^^^^\n",
      "TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 278, in dump_stream\n",
      "    bytes = self.serializer.dumps(vs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 469, in dumps\n",
      "    raise pickle.PicklingError(msg)\n",
      "_pickle.PicklingError: Could not serialize object: TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:224)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n",
      "Driver stacktrace:)\n",
      "23/12/02 15:42:58 WARN TaskSetManager: Lost task 7.0 in stage 3240.0 (TID 6305) (10.150.43.173 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 5 in stage 3240.0 failed 1 times, most recent failure: Lost task 5.0 in stage 3240.0 (TID 6303) (10.150.43.173 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 459, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle_fast.py\", line 632, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/mllib/recommendation.py\", line 51, in __reduce__\n",
      "    return Rating, (int(self.user), int(self.product), float(self.rating))\n",
      "                                    ^^^^^^^^^^^^^^^^^\n",
      "TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 278, in dump_stream\n",
      "    bytes = self.serializer.dumps(vs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 469, in dumps\n",
      "    raise pickle.PicklingError(msg)\n",
      "_pickle.PicklingError: Could not serialize object: TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:224)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n",
      "Driver stacktrace:)\n",
      "23/12/02 15:42:58 WARN TaskSetManager: Lost task 3.0 in stage 3240.0 (TID 6301) (10.150.43.173 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 5 in stage 3240.0 failed 1 times, most recent failure: Lost task 5.0 in stage 3240.0 (TID 6303) (10.150.43.173 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 459, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle_fast.py\", line 632, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/mllib/recommendation.py\", line 51, in __reduce__\n",
      "    return Rating, (int(self.user), int(self.product), float(self.rating))\n",
      "                                    ^^^^^^^^^^^^^^^^^\n",
      "TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 278, in dump_stream\n",
      "    bytes = self.serializer.dumps(vs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 469, in dumps\n",
      "    raise pickle.PicklingError(msg)\n",
      "_pickle.PicklingError: Could not serialize object: TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:224)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n",
      "Driver stacktrace:)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2256.trainALSModel.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 3240.0 failed 1 times, most recent failure: Lost task 5.0 in stage 3240.0 (TID 6303) (10.150.43.173 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 459, in dumps\n    return cloudpickle.dumps(obj, pickle_protocol)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n    cp.dump(obj)\n  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle_fast.py\", line 632, in dump\n    return Pickler.dump(self, obj)\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/mllib/recommendation.py\", line 51, in __reduce__\n    return Rating, (int(self.user), int(self.product), float(self.rating))\n                                    ^^^^^^^^^^^^^^^^^\nTypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 278, in dump_stream\n    bytes = self.serializer.dumps(vs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 469, in dumps\n    raise pickle.PicklingError(msg)\n_pickle.PicklingError: Could not serialize object: TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:224)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1293)\n\tat org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:988)\n\tat org.apache.spark.mllib.recommendation.ALS.run(ALS.scala:256)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainALSModel(PythonMLLibAPI.scala:485)\n\tat jdk.internal.reflect.GeneratedMethodAccessor108.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 459, in dumps\n    return cloudpickle.dumps(obj, pickle_protocol)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n    cp.dump(obj)\n  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle_fast.py\", line 632, in dump\n    return Pickler.dump(self, obj)\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/mllib/recommendation.py\", line 51, in __reduce__\n    return Rating, (int(self.user), int(self.product), float(self.rating))\n                                    ^^^^^^^^^^^^^^^^^\nTypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 278, in dump_stream\n    bytes = self.serializer.dumps(vs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 469, in dumps\n    raise pickle.PicklingError(msg)\n_pickle.PicklingError: Could not serialize object: TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:224)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/home/harsh-raj/Desktop/Big_Data_Analytics/Project2/movie_recommendation_using_ALS.ipynb Cell 48\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/harsh-raj/Desktop/Big_Data_Analytics/Project2/movie_recommendation_using_ALS.ipynb#X65sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m my_favorite_movies \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mLoki\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/harsh-raj/Desktop/Big_Data_Analytics/Project2/movie_recommendation_using_ALS.ipynb#X65sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# get recommends\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/harsh-raj/Desktop/Big_Data_Analytics/Project2/movie_recommendation_using_ALS.ipynb#X65sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m recommends \u001b[39m=\u001b[39m make_recommendation(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/harsh-raj/Desktop/Big_Data_Analytics/Project2/movie_recommendation_using_ALS.ipynb#X65sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     best_model_params\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39miterations\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m10\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mrank\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m26\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mlambda_\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m0.5\u001b[39;49m}, \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/harsh-raj/Desktop/Big_Data_Analytics/Project2/movie_recommendation_using_ALS.ipynb#X65sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     ratings_data\u001b[39m=\u001b[39;49mrating_data, \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/harsh-raj/Desktop/Big_Data_Analytics/Project2/movie_recommendation_using_ALS.ipynb#X65sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     df_movies\u001b[39m=\u001b[39;49mmovies, \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/harsh-raj/Desktop/Big_Data_Analytics/Project2/movie_recommendation_using_ALS.ipynb#X65sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     fav_movie_list\u001b[39m=\u001b[39;49mmy_favorite_movies, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/harsh-raj/Desktop/Big_Data_Analytics/Project2/movie_recommendation_using_ALS.ipynb#X65sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     n_recommendations\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/harsh-raj/Desktop/Big_Data_Analytics/Project2/movie_recommendation_using_ALS.ipynb#X65sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     spark_context\u001b[39m=\u001b[39;49msc)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/harsh-raj/Desktop/Big_Data_Analytics/Project2/movie_recommendation_using_ALS.ipynb#X65sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mRecommendations for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(my_favorite_movies[\u001b[39m0\u001b[39m]))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/harsh-raj/Desktop/Big_Data_Analytics/Project2/movie_recommendation_using_ALS.ipynb#X65sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, name \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(recommends):\n",
      "\u001b[1;32m/home/harsh-raj/Desktop/Big_Data_Analytics/Project2/movie_recommendation_using_ALS.ipynb Cell 48\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/harsh-raj/Desktop/Big_Data_Analytics/Project2/movie_recommendation_using_ALS.ipynb#X65sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m train_data \u001b[39m=\u001b[39m add_new_user_to_data(ratings_data, movieId_list, spark_context)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/harsh-raj/Desktop/Big_Data_Analytics/Project2/movie_recommendation_using_ALS.ipynb#X65sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m \u001b[39m# train best ALS\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/harsh-raj/Desktop/Big_Data_Analytics/Project2/movie_recommendation_using_ALS.ipynb#X65sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m model \u001b[39m=\u001b[39m ALS\u001b[39m.\u001b[39;49mtrain(\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/harsh-raj/Desktop/Big_Data_Analytics/Project2/movie_recommendation_using_ALS.ipynb#X65sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m     ratings\u001b[39m=\u001b[39;49mtrain_data,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/harsh-raj/Desktop/Big_Data_Analytics/Project2/movie_recommendation_using_ALS.ipynb#X65sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m     iterations\u001b[39m=\u001b[39;49mbest_model_params\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39miterations\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/harsh-raj/Desktop/Big_Data_Analytics/Project2/movie_recommendation_using_ALS.ipynb#X65sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m     rank\u001b[39m=\u001b[39;49mbest_model_params\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mrank\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/harsh-raj/Desktop/Big_Data_Analytics/Project2/movie_recommendation_using_ALS.ipynb#X65sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m     lambda_\u001b[39m=\u001b[39;49mbest_model_params\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mlambda_\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/harsh-raj/Desktop/Big_Data_Analytics/Project2/movie_recommendation_using_ALS.ipynb#X65sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m     seed\u001b[39m=\u001b[39;49m\u001b[39m99\u001b[39;49m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/harsh-raj/Desktop/Big_Data_Analytics/Project2/movie_recommendation_using_ALS.ipynb#X65sZmlsZQ%3D%3D?line=116'>117</a>\u001b[0m \u001b[39m# get inference rdd\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/harsh-raj/Desktop/Big_Data_Analytics/Project2/movie_recommendation_using_ALS.ipynb#X65sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m inference_rdd \u001b[39m=\u001b[39m get_inference_data(ratings_data, df_movies, movieId_list)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyspark/mllib/recommendation.py:298\u001b[0m, in \u001b[0;36mALS.train\u001b[0;34m(cls, ratings, rank, iterations, lambda_, blocks, nonnegative, seed)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    254\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\n\u001b[1;32m    255\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    262\u001b[0m     seed: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    263\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m MatrixFactorizationModel:\n\u001b[1;32m    264\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39m    Train a matrix factorization model given an RDD of ratings by users\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[39m    for a subset of products. The ratings matrix is approximated as the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39m        (default: None)\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 298\u001b[0m     model \u001b[39m=\u001b[39m callMLlibFunc(\n\u001b[1;32m    299\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mtrainALSModel\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    300\u001b[0m         \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare(ratings),\n\u001b[1;32m    301\u001b[0m         rank,\n\u001b[1;32m    302\u001b[0m         iterations,\n\u001b[1;32m    303\u001b[0m         lambda_,\n\u001b[1;32m    304\u001b[0m         blocks,\n\u001b[1;32m    305\u001b[0m         nonnegative,\n\u001b[1;32m    306\u001b[0m         seed,\n\u001b[1;32m    307\u001b[0m     )\n\u001b[1;32m    308\u001b[0m     \u001b[39mreturn\u001b[39;00m MatrixFactorizationModel(model)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyspark/mllib/common.py:139\u001b[0m, in \u001b[0;36mcallMLlibFunc\u001b[0;34m(name, *args)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39massert\u001b[39;00m sc\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    138\u001b[0m api \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(sc\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mPythonMLLibAPI(), name)\n\u001b[0;32m--> 139\u001b[0m \u001b[39mreturn\u001b[39;00m callJavaFunc(sc, api, \u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyspark/mllib/common.py:131\u001b[0m, in \u001b[0;36mcallJavaFunc\u001b[0;34m(sc, func, *args)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Call Java Function\"\"\"\u001b[39;00m\n\u001b[1;32m    130\u001b[0m java_args \u001b[39m=\u001b[39m [_py2java(sc, a) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m args]\n\u001b[0;32m--> 131\u001b[0m \u001b[39mreturn\u001b[39;00m _java2py(sc, func(\u001b[39m*\u001b[39;49mjava_args))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    180\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2256.trainALSModel.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 3240.0 failed 1 times, most recent failure: Lost task 5.0 in stage 3240.0 (TID 6303) (10.150.43.173 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 459, in dumps\n    return cloudpickle.dumps(obj, pickle_protocol)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n    cp.dump(obj)\n  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle_fast.py\", line 632, in dump\n    return Pickler.dump(self, obj)\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/mllib/recommendation.py\", line 51, in __reduce__\n    return Rating, (int(self.user), int(self.product), float(self.rating))\n                                    ^^^^^^^^^^^^^^^^^\nTypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 278, in dump_stream\n    bytes = self.serializer.dumps(vs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 469, in dumps\n    raise pickle.PicklingError(msg)\n_pickle.PicklingError: Could not serialize object: TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:224)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1293)\n\tat org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:988)\n\tat org.apache.spark.mllib.recommendation.ALS.run(ALS.scala:256)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainALSModel(PythonMLLibAPI.scala:485)\n\tat jdk.internal.reflect.GeneratedMethodAccessor108.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 459, in dumps\n    return cloudpickle.dumps(obj, pickle_protocol)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n    cp.dump(obj)\n  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle_fast.py\", line 632, in dump\n    return Pickler.dump(self, obj)\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/mllib/recommendation.py\", line 51, in __reduce__\n    return Rating, (int(self.user), int(self.product), float(self.rating))\n                                    ^^^^^^^^^^^^^^^^^\nTypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 278, in dump_stream\n    bytes = self.serializer.dumps(vs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/harsh-raj/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 469, in dumps\n    raise pickle.PicklingError(msg)\n_pickle.PicklingError: Could not serialize object: TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:224)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# my favorite movies\n",
    "my_favorite_movies = ['Loki']\n",
    "\n",
    "# get recommends\n",
    "recommends = make_recommendation(\n",
    "    best_model_params={'iterations': 10, 'rank': 26, 'lambda_': 0.5}, \n",
    "    ratings_data=rating_data, \n",
    "    df_movies=movies, \n",
    "    fav_movie_list=my_favorite_movies, \n",
    "    n_recommendations=10, \n",
    "    spark_context=sc)\n",
    "\n",
    "print('Recommendations for {}:'.format(my_favorite_movies[0]))\n",
    "for i, name in enumerate(recommends):\n",
    "    print('{0}: {1}'.format(i+1, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list of movie recommendations look completely different than the list from my previous **KNN** model recommender. Not only it recommends movies outside of years between 2007 and 2009 periods, but also recommends movies that were less known. So this can offer users some elements of suprise so that users won't get bored by getting the same popular movies all the time.\n",
    "\n",
    "So this list of recommendations can be blended into the previous list of recommendations from **KNN** model recommender"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
